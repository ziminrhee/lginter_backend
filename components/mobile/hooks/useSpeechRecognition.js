import { useCallback, useState } from 'react';

export default function useSpeechRecognition({ onResult, onInterim, onStart, onError, onEnd } = {}) {
  const [isListening, setIsListening] = useState(false);

  const startVoiceRecognition = useCallback(() => {
    if (typeof window === 'undefined') return;
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      alert('âŒ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ–Šï¸');
      return;
    }

    try {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
      recognition.lang = 'ko-KR';
      recognition.continuous = true; // allow interim updates
      recognition.interimResults = true;
      recognition.maxAlternatives = 1;

      recognition.onstart = () => {
        setIsListening(true);
        if (typeof window !== 'undefined') window.isListening = true;
        if (typeof onInterim === 'function') onInterim('');
        if (typeof onStart === 'function') onStart();
        console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ë¨');
      };

      recognition.onresult = (event) => {
        // Display text: concat every hypothesis so far (super-fast feedback)
        const displayText = Array.from(event.results)
          .map(r => r[0]?.transcript ?? '')
          .join('')
          .trim();
        if (displayText && typeof onInterim === 'function') onInterim(displayText);

        // Collect final segments (if any appeared in this event)
        let finalText = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const res = event.results[i];
          if (res.isFinal) finalText += res[0].transcript;
        }
        if (finalText) {
          const last = event.results[event.results.length - 1][0];
          const confidence = last?.confidence ?? 1;
          if (typeof onResult === 'function') onResult({ transcript: finalText.trim(), confidence });
          try { recognition.stop(); } catch (_) {}
          setTimeout(() => {
            const submitBtn = document.querySelector('button[type="submit"]');
            if (submitBtn) submitBtn.click();
          }, 300);
        }
      };

      recognition.onerror = (event) => {
        console.error('âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error);
        console.log(event);
        setIsListening(false);
        if (typeof window !== 'undefined') window.isListening = false;
        if (typeof onError === 'function') onError(event.error);

        let errorMsg = '';
        if (event.error === 'no-speech') {
          errorMsg = 'ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ–Šï¸';
        } else if (event.error === 'not-allowed') {
          errorMsg = 'âš ï¸ ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜, ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.\n\nğŸ’¡ íŒ: HTTP ì—°ê²°ì—ì„œëŠ” ë³´ì•ˆìƒ ë§ˆì´í¬ê°€ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.';
        } else if (event.error === 'network') {
          errorMsg = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ–Šï¸';
        } else {
          errorMsg = 'ìŒì„± ì¸ì‹ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ–Šï¸';
        }
        if (errorMsg) alert(errorMsg);
      };

      recognition.onend = () => {
        setIsListening(false);
        if (typeof window !== 'undefined') window.isListening = false;
        if (typeof onEnd === 'function') onEnd();
        console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œ');
      };

      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹œë„...');
      recognition.start();
    } catch (error) {
      console.error('ìŒì„± ì¸ì‹ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      alert('ìŒì„± ì¸ì‹ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ–Šï¸');
      setIsListening(false);
    }
  }, [onResult, onStart, onError, onEnd]);

  return { isListening, startVoiceRecognition };
}


